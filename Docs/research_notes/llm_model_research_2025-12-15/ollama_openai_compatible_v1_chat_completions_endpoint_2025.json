{
  "query": "Ollama OpenAI compatible v1 chat completions endpoint 2025",
  "answer": "Here’s the 2025 quick-start for Ollama’s OpenAI‑compatible v1 chat completions endpoint.\n\nEndpoint and base URL\n- POST http://localhost:11434/v1/chat/completions\n- When using the OpenAI SDKs, set base_url to http://localhost:11434/v1 and any placeholder API key (it’s required by the client but ignored by Ollama). ([ollama.com](https://ollama.com/blog/openai-compatibility))\n\nWhat’s supported (Dec 2025)\n- Core: chat completions, streaming, JSON mode, seed/reproducible outputs, vision (text+image messages), tools (function calling), logprobs. ([docs.ollama.com](https://docs.ollama.com/api/openai-compatibility))\n- Request fields you can rely on: model, messages (including array-of-parts with text and image_url/base64), temperature, top_p, max_tokens, stop, response_format, seed, stream (+ stream_options.include_usage), tools, tool_choice, logit_bias, user, n. ([docs.ollama.com](https://docs.ollama.com/api/openai-compatibility))\n- Tip: if a client expects an OpenAI model name (e.g., gpt‑3.5‑turbo), alias a local model: ollama cp llama3.2 gpt-3.5-turbo. ([docs.ollama.com](https://docs.ollama.com/api/openai-compatibility))\n- Also available in 2025: the OpenAI /v1/responses API (non‑stateful flavor) if you prefer that style. ([docs.ollama.com](https://docs.ollama.com/api/openai-compatibility))\n\nMinimal examples\n- curl:\n  curl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"model\": \"llama3.2\",\n      \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n    }'\n  ([ollama.com](https://ollama.com/blog/openai-compatibility))\n\n- Python (OpenAI SDK 1.x):\n  from openai import OpenAI\n  client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n  resp = client.chat.completions.create(\n      model=\"llama3.2\",\n      messages=[{\"role\":\"user\",\"content\":\"Say this is a test\"}]\n  )\n  print(resp.choices[0].message.content)\n  ([docs.ollama.com](https://docs.ollama.com/api/openai-compatibility))\n\n- JavaScript (openai npm):\n  import OpenAI from \"openai\";\n  const openai = new OpenAI({ baseURL: \"http://localhost:11434/v1\", apiKey: \"ollama\" });\n  const completion = await openai.chat.completions.create({\n    model: \"llama3.2\",\n    messages: [{ role: \"user\", content: \"Why is the sky blue?\" }],\n  });\n  console.log(completion.choices[0].message.content);\n  ([ollama.com](https://ollama.com/blog/openai-compatibility))\n\nTools (function calling)\n- Pass OpenAI‑style tools in the request; Ollama’s OpenAI‑compatible endpoint supports them. Example (Python OpenAI SDK):\n  response = client.chat.completions.create(\n    model=\"llama3.1\",\n    messages=messages,\n    tools=[{\"type\":\"function\",\"function\":{\"name\":\"get_current_weather\",\"parameters\":{\"type\":\"object\",\"properties\":{\"city\":{\"type\":\"string\"}},\"required\":[\"city\"]}}}],\n  )\n  ([ollama.com](https://ollama.com/blog/tool-support))\n- Note: streaming + tool calls has been evolving; some clients may not receive streamed tool call deltas as they would with OpenAI. If you hit issues, test without stream=True or follow active discussions. ([github.com](https://github.com/ollama/ollama/issues/12557?utm_source=openai))\n\nVision\n- Send messages as an array of parts with text and an image_url (or base64). ([docs.ollama.com](https://docs.ollama.com/api/openai-compatibility))\n\nWant this tailored to a specific language/runtime, or need a streaming example (SSE) or JSON‑mode snippet?",
  "citations": [],
  "sources": [],
  "metadata": {}
}

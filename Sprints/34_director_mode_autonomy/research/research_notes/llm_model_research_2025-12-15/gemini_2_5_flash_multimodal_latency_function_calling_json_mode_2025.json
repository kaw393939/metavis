{
  "query": "Gemini 2.5 flash multimodal latency function calling JSON mode 2025",
  "answer": "Here’s a 2025 cheat sheet for “Gemini 2.5 Flash — multimodal, latency, function calling, JSON mode.”\n\nWhat it is (as of Dec 16, 2025)\n- Model: gemini-2.5-flash. Inputs: text, images, video, audio. Output: text. Context window: up to ~1,048,576 input tokens, 65,536 output tokens. Knowledge cutoff: Jan 2025. ([ai.google.dev](https://ai.google.dev/gemini-api/docs/models?utm_source=openai))\n- Positioning: low-latency, price/performance “Flash” model; natively multimodal; supports “thinking budgets” to trade quality vs speed/cost. ([deepmind.google](https://deepmind.google/en/models/gemini/flash/?utm_source=openai))\n\nLatency in practice\n- Designed for low latency; you can reduce latency further by lowering or disabling the thinking budget (e.g., set thinkingBudget=0), or use Flash‑Lite for the fastest responses. Live variants target real‑time voice/video. ([blog.google](https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025?utm_source=openai))\n- Google does not publish a universal SLA for response time. Third‑party benchmarks vary by workload; for example, AutoBench recently measured average end‑to‑end latency ~48.7 s for gemini‑2.5‑flash on its mixed evals (not official; depends heavily on prompt, media size, region, and budget). ([autobench.org](https://autobench.org/models/gemini-2.5-flash?utm_source=openai))\n\nFunction calling (tools)\n- Supported, including parallel and compositional calling; configurable modes (AUTO/ANY/NONE). Provide tool schemas via functionDeclarations; app executes the functions and can feed results back to the model. ([ai.google.dev](https://ai.google.dev/guide/function_calling))\n\nStructured outputs (“JSON mode”)\n- Strict JSON output is supported via structured outputs: set response_mime_type to application/json and pass a response_json_schema (JSON Schema). Works in the REST API and Google’s SDKs; supports Pydantic/Zod. ([ai.google.dev](https://ai.google.dev/gemini-api/docs/structured-output))\n\nTiny code snippets\n\n- Function calling (JavaScript)\n  import { GoogleGenAI, Type } from '@google/genai';\n  const ai = new GoogleGenAI({});\n  const tool = {\n    name: 'get_weather',\n    description: 'Return weather for a city.',\n    parameters: { type: Type.OBJECT, properties: { city: { type: Type.STRING } }, required: ['city'] }\n  };\n  const res = await ai.models.generateContent({\n    model: 'gemini-2.5-flash',\n    contents: 'What is the weather in Boston?',\n    config: { tools: [{ functionDeclarations: [tool] }] }\n  });\n  // If res.functionCalls exists, execute get_weather(res.functionCalls[0].args.city) and send result back\n  // in the next turn. ([ai.google.dev](https://ai.google.dev/guide/function_calling))\n\n- Strict JSON output (Python)\n  from google import genai\n  client = genai.Client()\n  schema = {\n    \"type\":\"object\",\n    \"properties\":{\n      \"title\":{\"type\":\"string\"},\n      \"summary\":{\"type\":\"string\"},\n      \"tags\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}}\n    },\n    \"required\":[\"title\",\"summary\"]\n  }\n  resp = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Summarize this release note and propose tags: ...\",\n    config={\"response_mime_type\":\"application/json\",\"response_json_schema\":schema}\n  )\n  print(resp.text)  # Valid JSON per schema. ([ai.google.dev](https://ai.google.dev/gemini-api/docs/structured-output))\n\nMultimodal tips\n- For images/video/audio, pass media parts alongside text; 2.5 Flash accepts all three as inputs (output is text). Use Flash‑Image if you need image generation/editing. ([ai.google.dev](https://ai.google.dev/gemini-api/docs/models?utm_source=openai))\n\nIf you tell me your SDK (Node, Python, REST) and target latency budget, I can tailor a minimal template (with thinkingBudget, streaming, and tool config) for your use case.",
  "citations": [],
  "sources": [],
  "metadata": {}
}

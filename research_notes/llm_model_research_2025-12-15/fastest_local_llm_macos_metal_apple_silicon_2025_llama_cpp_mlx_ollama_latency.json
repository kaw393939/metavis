{
  "query": "fastest local llm macos metal apple silicon 2025 llama.cpp mlx ollama latency",
  "answer": "Shortest answer\n- Fastest stack on Apple Silicon in late‑2025 for low latency: MLX‑LM (MLX models) ≈ llama.cpp with FlashAttention; Ollama is easiest but typically slower and more variable. On an M‑series desktop (e.g., M2 Ultra), MLX leads on sustained tokens/sec; llama.cpp shines on prompt‑eval/TTFT; Ollama generally lags both. ([arxiv.org](https://arxiv.org/abs/2511.05502?utm_source=openai))\n\nWhat to use (and how) on macOS + Metal\n- MLX‑LM (Apple’s MLX)\n  - Why: Highest steady‑state throughput on Apple Silicon; clean Metal path; great with long prompts. ([arxiv.org](https://arxiv.org/abs/2511.05502?utm_source=openai))\n  - Install/run: pip install mlx-lm; then e.g. mlx_lm.chat --model mlx-community/Llama-3.2-3B-Instruct-4bit. If model size ≈ RAM, raise the wired-memory limit for big speedups on macOS 15+: sudo sysctl iogpu.wired_limit_mb=<MB>. ([github.com](https://github.com/ml-explore/mlx-lm?utm_source=openai))\n- llama.cpp (Metal backend)\n  - Why: Excellent latency and prompt‑eval speed; competitive decode throughput with FA on. ([reddit.com](https://www.reddit.com/r/LocalLLaMA/comments/1h01719?utm_source=openai))\n  - Build/run: compile with Metal, then offload everything and enable FA:\n    - Build: LLAMA_METAL=1 make -j\n    - Run: ./llama-cli -m model.gguf -ngl 999 -fa -b 512 -ub 256 -c 8192 -t <P‑core count>\n    - Tuning: lift -b until no gains; keep context (-c) as low as acceptable; always use full GPU offload (-ngl 999) on Apple Silicon. ([eduardstal.com](https://eduardstal.com/blog/03-2025_running-llama-on-apple-silicon?utm_source=openai))\n- Ollama\n  - Why: Best ergonomics/CLI/API, but performance trails MLX/llama.cpp and can regress across versions; enable Metal features explicitly. ([arxiv.org](https://arxiv.org/abs/2511.05502?utm_source=openai))\n  - Turn on the fast path: set OLLAMA_FLASH_ATTENTION=1 and (optionally) OLLAMA_KV_CACHE_TYPE=q8_0 before starting the server. Use the native installer on macOS to avoid missing GPU deps. ([docs.ollama.com](https://docs.ollama.com/faq?utm_source=openai))\n\nRealistic 2025 speeds (decode, tokens/sec) you can expect\n- Llama 3.x 8B Q4_K_M on M2 family: ~22–60 tok/s (llama.cpp/MLX), with 500–1250 tok/s prompt‑eval depending on chip and settings. 70B Q4_K_M on M2 Ultra: ~12–20 tok/s. ([eduardstal.com](https://eduardstal.com/blog/03-2025_running-llama-on-apple-silicon?utm_source=openai))\n- Community reports on M4‑class: 14B models ~13–33 tok/s; 32B ~8–16 tok/s depending on quant/context. Treat as ballpark only. ([ricklan.net](https://ricklan.net/blog/2025/03/benchmark-llm-on-apple-silicon/?utm_source=openai))\n- Note: Newer chips improve TTFT more than decode (decode is bandwidth‑bound). Apple claims M5 cuts TTFT 3.3–4× vs M4 and adds ~19–27% decode speed from higher memory bandwidth. ([dataconomy.com](https://dataconomy.com/2025/11/21/apple-claims-m5-runs-ai-models-nearly-30-percent-faster-than-m4?utm_source=openai))\n\nQuick picks by goal\n- Lowest interactive latency (chat, coding): llama.cpp with -fa -ngl 999; keep context modest; Q4_K_M or Q5_K_M; set threads to P‑cores. ([eduardstal.com](https://eduardstal.com/blog/03-2025_running-llama-on-apple-silicon?utm_source=openai))\n- Highest steady throughput (summaries, long docs): MLX‑LM models (mlx-community) with wired memory raised on macOS 15+. ([github.com](https://github.com/ml-explore/mlx-lm?utm_source=openai))\n- Easiest API/server: Ollama, but enable FA and KV‑quant; if speed-critical, consider swapping to llama.cpp/MLX behind your app. ([docs.ollama.com](https://docs.ollama.com/faq?utm_source=openai))\n\nCommon gotchas on Macs\n- GPU path must actually be active: use Metal builds (llama.cpp) or MLX models. Reinstalling Ollama via the official .pkg (not Homebrew) can fix missing GPU deps. ([reddit.com](https://www.reddit.com/r/ollama/comments/1h7grjl?utm_source=openai))\n- KV cache and FA: KV quantization needs FA on in llama.cpp/Ollama; q8_0 is a safe default, but some models (e.g., Gemma 3) show regressions with KV quant in Ollama—disable if you see slowdowns. ([github.com](https://github.com/ggml-org/llama.cpp/issues/10378?utm_source=openai))\n- Big contexts tank speed; use the smallest context your app tolerates. ([eduardstal.com](https://eduardstal.com/blog/03-2025_running-llama-on-apple-silicon?utm_source=openai))\n\nIf you share your Mac model (chip/RAM), target model sizes, and latency vs throughput priority, I’ll give you concrete flags and model picks for your setup.",
  "citations": [],
  "sources": [],
  "metadata": {}
}

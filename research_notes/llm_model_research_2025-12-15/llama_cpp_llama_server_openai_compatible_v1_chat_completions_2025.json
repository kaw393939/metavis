{
  "query": "llama.cpp llama-server OpenAI compatible /v1/chat/completions 2025",
  "answer": "Here’s the short version for 2025: llama.cpp’s llama-server exposes OpenAI‑compatible endpoints, including /v1/chat/completions. You can point OpenAI SDKs or curl at it by setting base_url to your server and using any API key (or none, if you don’t enable auth).\n\nQuick start\n- Build and run the server\n  - Build: cmake -B build && cmake --build build --target llama-server. ([docfork.com](https://docfork.com/ggml-org/llama.cpp?utm_source=openai))\n  - Run a model and give it a friendly model id with --alias (that’s the value clients must send as model): \n    ./build/bin/llama-server -m /path/model.gguf -a my-model --port 8080\n    The server exposes a web UI at http://localhost:8080 and OpenAI‑style routes under /v1. ([github.com](https://github.com/osllmai/llama.cpp?utm_source=openai))\n  - The --alias flag sets the model name returned by the server; otherwise /v1/models may show the GGUF path. ([lists.debian.org](https://lists.debian.org/debian-user/2024/07/msg00421.html?utm_source=openai))\n\n- Call /v1/chat/completions\n  - curl:\n    curl http://localhost:8080/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"model\":\"my-model\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello!\"}]}'\n    This is the OpenAI‑compatible chat endpoint. ([github.com](https://github.com/osllmai/llama.cpp?utm_source=openai))\n  - Python (OpenAI SDK):\n    from openai import OpenAI\n    client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"not-needed\")\n    r = client.chat.completions.create(model=\"my-model\", messages=[{\"role\":\"user\",\"content\":\"Hello!\"}])\n    print(r.choices[0].message.content)\n    Same request shape as OpenAI; only base_url and (dummy) key differ. ([git.cturan.dev](https://git.cturan.dev/cturan/llama.cpp/src/a2b0fe8d378ef9e7e5735329181e6bdcd23a08ac/tools/server/README.md?utm_source=openai))\n  - Streaming: set \"stream\": true in the payload; the server uses SSE like OpenAI’s. ([deepwiki.com](https://deepwiki.com/ggml-org/llama.cpp/7-advanced-features?utm_source=openai))\n\nOther useful endpoints\n- /v1/completions (text), /v1/embeddings (embeddings), GET /v1/models (model listing). ([deepwiki.com](https://deepwiki.com/ggml-org/llama.cpp/5.2-http-server?utm_source=openai))\n\nNotes and 2025 gotchas\n- response_format json/schema: support exists but has had bugs this year; if you hit issues, consider simple \"json_object\" or grammar-based constraints as a fallback. ([fossies.org](https://fossies.org/linux/llama.cpp/tools/server/README.md?utm_source=openai))\n- Function/tool calling: supported when you start the server with --jinja and (if needed) supply an appropriate chat template; performance can be lower on some models when tools are enabled. ([fossies.org](https://fossies.org/linux/llama.cpp/docs/function-calling.md?utm_source=openai))\n- Concurrency and batching: the server supports multiple “slots” for parallel requests (configure with --n-parallel/--parallel). ([deepwiki.com](https://deepwiki.com/ggml-org/llama.cpp/2.2-basic-usage?utm_source=openai))\n\nWant me to tailor exact commands for your OS/GPU, enable streaming, or wire this into a specific OpenAI client (Python/JS)?",
  "citations": [],
  "sources": [],
  "metadata": {}
}

{
  "query": "MLX local LLM server OpenAI compatible 2025",
  "answer": "Here are the easiest ways (as of December 16, 2025) to run a local, MLX‑backed LLM server on Apple‑silicon Macs that speaks the OpenAI API.\n\nFast, GUI “all‑in‑one”\n- Pico AI Server (Mac App Store): MLX‑accelerated local server with a web UI and OpenAI‑compatible endpoints (also supports an Ollama‑style API). Good if you want zero setup and multi‑user. ([apps.apple.com](https://apps.apple.com/us/app/pico-ai-homelab-llm-server/id6738607769?utm_source=openai))\n\nOpen‑source CLIs (drop‑in OpenAI API)\n- mlx‑omni‑server: A Python server built on MLX with OpenAI and Anthropic‑compatible endpoints; supports chat, audio (TTS/STT), image gen, and embeddings. Quick start:\n  - pip install mlx-omni-server\n  - Start: mlx-omni-server\n  - Use with OpenAI SDK:\n    - base_url=http://localhost:10240/v1 and any dummy api_key\n  - Example model: mlx-community/gemma-3-1b-it-4bit-DWQ\n  This is the most “batteries‑included” OSS option. ([github.com](https://github.com/madroidmaq/mlx-omni-server?utm_source=openai))\n\n- mlx‑openai‑server: FastAPI server focused on OpenAI compatibility for MLX models; supports chat (streaming), function/tools, embeddings, multimodal (text, images, audio), and Flux‑series image gen/editing.\n  - Install: pip install mlx-openai-server\n  - Launch (text-only): mlx-openai-server launch --model-path <mlx-model> --model-type lm\n  - Launch (multimodal): mlx-openai-server launch --model-path <mlx-model> --model-type multimodal\n  - Then point your OpenAI SDK at base_url=http://127.0.0.1:8000/v1 (default). ([github.com](https://github.com/cubist38/mlx-openai-server?utm_source=openai))\n\n“Upstream” MLX option\n- mlx‑lm (Apple’s MLX tooling) includes a simple OpenAI‑compatible server command. Quick start:\n  - pip install mlx-lm openai\n  - python -m mlx_lm server --model mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit --host 0.0.0.0 --port 8080\n  - Then use OpenAI’s client with base_url=http://127.0.0.1:8080/v1 and any api_key\n  Notes: choose a model that fits your RAM; set --max-tokens appropriately for your context needs. ([github.com](https://github.com/aider-ai/aider/issues/4526?utm_source=openai))\n\nMinimal client test (works with any of the above)\n- Python:\n  from openai import OpenAI\n  client = OpenAI(base_url=\"http://127.0.0.1:8000/v1\", api_key=\"local\")\n  out = client.chat.completions.create(\n      model=\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n      messages=[{\"role\":\"user\",\"content\":\"Hello from MLX!\"}]\n  )\n  print(out.choices[0].message.content)\n\n- curl:\n  curl http://127.0.0.1:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" -H \"Authorization: Bearer local\" \\\n    -d '{\"model\":\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello!\"}]}'\n\nTip: Use MLX‑converted models (e.g., the mlx‑community org on Hugging Face). Some servers default to ports 8000 or 10240; mlx‑lm’s built‑in server often uses 8080—match your client base_url accordingly. If you need audio or image endpoints, prefer mlx‑omni‑server or mlx‑openai‑server. ([github.com](https://github.com/madroidmaq/mlx-omni-server?utm_source=openai))\n\nWant a one‑liner recommendation? Tell me:\n- Your Mac model and RAM (e.g., M3 Pro/36 GB vs M3 Ultra/128 GB).\n- Whether you need chat only, or also images/audio.\n- CLI vs GUI preference.",
  "citations": [],
  "sources": [],
  "metadata": {}
}
